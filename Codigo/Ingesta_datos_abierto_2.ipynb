{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "968824ad-a146-48e5-83d3-d2a86ec2c60a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Cuaderno de ingesta de datos\n",
    "\n",
    "En este bloque traeremos la información de datos abiertos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a77ebd6-0dd7-4565-b553-b8cfa7c385af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1: Descargar los datos con requests y leerlos en pandas\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c034b561-6862-4f71-b034-d092446d3999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE CATALOG main\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9c9c617-a76e-48ae-8bf9-eb01822c83f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.catalog.listDatabases()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2b8aea7-3fc2-49f9-8a5c-9270ff6654da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Descargar MEN\n",
    "url_men = \"https://www.datos.gov.co/resource/nudc-7mev.csv?$limit=100000\"\n",
    "\n",
    "response_men = requests.get(url_men)\n",
    "df_men_pd = pd.read_csv(StringIO(response_men.text))\n",
    "df_men = spark.createDataFrame(df_men_pd)\n",
    "\n",
    "display(df_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dc0eab9-498c-43d7-97d2-246229a710c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS main.diplomado_datos\")\n",
    "\n",
    "df_men.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"main.diplomado_datos.men_estadisticas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a98abb8-1f28-4566-9f15-92059cc96a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_registros = 19449349\n",
    "limite = 50000     \n",
    "paginas = (total_registros // limite) + 1\n",
    "\n",
    "\n",
    "for i in range(paginas):\n",
    "    offset = i * limite\n",
    "    url_secop = f\"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit={limite}&$offset={offset}\"\n",
    "\n",
    "    print(f\"Descargando página {i+1}/{paginas} (offset={offset})...\")\n",
    "\n",
    "    response_secop = requests.get(url_secop)\n",
    "\n",
    "    if response_secop.status_code == 200:\n",
    "        df_secop_pd = pd.read_csv(StringIO(response_secop.text), dtype=str)\n",
    "\n",
    "        if df_secop_pd.empty:\n",
    "            print(f\"⚠️ Página {i+1} vacía. Fin de datos.\")\n",
    "            break\n",
    "\n",
    "        df_secop_spark = spark.createDataFrame(df_secop_pd)\n",
    "\n",
    "        df_secop_spark.write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(\"main.diplomado_datos.secop\")\n",
    "\n",
    "        print(f\"✓ Página {i+1} guardada con {len(df_secop_pd)} filas.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Error HTTP {response_secop.status_code}. Deteniendo proceso.\")\n",
    "        break\n",
    "\n",
    "print(\"✅ ¡Carga completa de SECOP en Delta!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a126f058-e6a9-4075-924c-9a2f3a3cee1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Offset donde falló\n",
    "offset_inicial = 6500000      # Página 131 (6500000 = (131-1)*50000)\n",
    "limite = 50000\n",
    "\n",
    "# Calcula cuántas páginas faltan:\n",
    "total_registros = 19449349\n",
    "paginas_faltantes = (total_registros - offset_inicial) // limite + 1\n",
    "\n",
    "print(f\"➡️ Vamos a descargar {paginas_faltantes} páginas (bloques de {limite}) empezando en offset {offset_inicial}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e8209e0-7560-4838-8eef-0563fda12904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# Loop para continuar descarga\n",
    "# --------------------------------------------\n",
    "\n",
    "for i in range(paginas_faltantes):\n",
    "    offset = offset_inicial + (i * limite)\n",
    "    url_secop = f\"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit={limite}&$offset={offset}\"\n",
    "    \n",
    "    print(f\"Descargando página {i+1} con offset={offset}...\")\n",
    "\n",
    "    response_secop = requests.get(url_secop)\n",
    "\n",
    "    if response_secop.status_code == 200:\n",
    "        df_secop_pd = pd.read_csv(StringIO(response_secop.text), dtype=str)\n",
    "\n",
    "        if df_secop_pd.empty:\n",
    "            print(f\"⚠️ Página {i+1} vacía. Fin de datos.\")\n",
    "            break\n",
    "\n",
    "        df_secop_spark = spark.createDataFrame(df_secop_pd)\n",
    "\n",
    "        # Graba en Delta en modo append\n",
    "        df_secop_spark.write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(\"main.diplomado_datos.secop\")\n",
    "\n",
    "        print(f\"✓ Página {i+1} guardada con {len(df_secop_pd)} filas.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Error HTTP {response_secop.status_code} en página {i+1}. Deteniendo proceso.\")\n",
    "        break\n",
    "\n",
    "print(\"✅ ¡Carga reanudada de SECOP completada!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta_datos_abierto_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
